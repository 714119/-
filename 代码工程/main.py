import yaml
from pathlib import Path
import sys
import os

print("=== Starting Traffic Analysis Pipeline ===\n")

current_dir = Path(__file__).parent
src_dir = current_dir / 'src'
sys.path.insert(0, str(src_dir))

print(f"Current directory: {current_dir}")
print(f"Source directory: {src_dir}")
print(f"Source exists: {src_dir.exists()}\n")

# Test imports one by one with detailed error messages
print("=== Testing Imports ===")

try:
    from data_preprocessing.largest_processor import LargeSTProcessor
    print("‚úÖ SUCCESS: LargeSTProcessor")
except Exception as e:
    print(f"‚ùå FAILED: LargeSTProcessor - {e}")
    sys.exit(1)

try:
    from data_preprocessing.noaa_processor import NOAAProcessor
    print("‚úÖ SUCCESS: NOAAProcessor")
except Exception as e:
    print(f"‚ùå FAILED: NOAAProcessor - {e}")
    sys.exit(1)

try:
    from data_preprocessing.twitter_processor import TwitterProcessor
    print("‚úÖ SUCCESS: TwitterProcessor")
except Exception as e:
    print(f"‚ùå FAILED: TwitterProcessor - {e}")
    sys.exit(1)

try:
    from data_preprocessing.data_fusion import DataFusion
    print("‚úÖ SUCCESS: DataFusion")
except Exception as e:
    print(f"‚ùå FAILED: DataFusion - {e}")
    sys.exit(1)

print("\n‚úÖ ALL IMPORTS SUCCESSFUL!\n")

def main():
    """Main data processing pipeline"""
    print("üöÄ Starting multi-source data processing pipeline...")
    
    # Load configuration
    try:
        with open('config.yaml', 'r') as f:
            config = yaml.safe_load(f)
        print("‚úÖ Config loaded successfully")
    except FileNotFoundError:
        print("‚ùå config.yaml not found. Using default configuration.")
        config = {
            'data_paths': {
                'largest_raw': 'data/raw/LargeST',
                'noaa_raw': 'data/raw/NOAA',
                'twitter_raw': 'data/raw/twitter',
                'processed': 'data/processed',
                'fused': 'data/fused'
            },
            'largest': {
                'target_cities': ['beijing', 'shanghai', 'guangzhou']
            }
        }
    
    # 1. Process traffic data
    print("\n" + "="*50)
    print("üìä STEP 1: Processing Traffic Data")
    print("="*50)
    
    traffic_processor = LargeSTProcessor()
    for city in config['largest']['target_cities']:
        print(f"\nProcessing {city}...")
        try:
            df = traffic_processor.load_largest_data(city)
            print(f"  Loaded {len(df)} records")
            
            df_processed = traffic_processor.preprocess_traffic_data(df)
            print(f"  Processed {len(df_processed)} records")
            
            traffic_processor.save_processed_data(df_processed, city)
            print(f"  ‚úÖ {city} traffic data saved")
        except Exception as e:
            print(f"  ‚ùå Error processing {city}: {e}")
    
    # 2. Process weather data
    print("\n" + "="*50)
    print("üå§Ô∏è  STEP 2: Processing Weather Data")
    print("="*50)
    
    weather_processor = NOAAProcessor()
    try:
        weather_df = weather_processor.load_noaa_data()
        print(f"Loaded {len(weather_df)} weather records")
        
        weather_processed = weather_processor.preprocess_weather_data(weather_df)
        print(f"Processed {len(weather_processed)} records")
        
        weather_resampled = weather_processor.resample_weather_data(weather_processed)
        print(f"Resampled to {len(weather_resampled)} records")
        
        output_path = Path(config['data_paths']['processed']) / "noaa_weather_processed.parquet"
        output_path.parent.mkdir(parents=True, exist_ok=True)
        weather_resampled.to_parquet(output_path, index=False)
        print(f"‚úÖ Weather data saved to {output_path}")
    except Exception as e:
        print(f"‚ùå Error processing weather data: {e}")
    
    # 3. Process Twitter data
    print("\n" + "="*50)
    print("üê¶ STEP 3: Processing Twitter Data")
    print("="*50)
    
    twitter_processor = TwitterProcessor()
    try:
        twitter_df = twitter_processor.load_twitter_data()
        print(f"Loaded {len(twitter_df)} Twitter records")
        
        events_processed = twitter_processor.preprocess_twitter_data(twitter_df)
        print(f"Processed {len(events_processed)} event records")
        
        events_output_path = Path(config['data_paths']['processed']) / "twitter_events_processed.parquet"
        events_processed.to_parquet(events_output_path, index=False)
        print(f"‚úÖ Twitter data saved to {events_output_path}")
    except Exception as e:
        print(f"‚ùå Error processing Twitter data: {e}")
    
    # 4. Data fusion
    print("\n" + "="*50)
    print("üîó STEP 4: Data Fusion")
    print("="*50)
    
    fusion = DataFusion()
    for city in config['largest']['target_cities']:
        print(f"\nFusing data for {city}...")
        try:
            fused_df = fusion.create_fused_dataset(city)
            if fused_df is not None and not fused_df.empty:
                print(f"  ‚úÖ {city}: Fused {len(fused_df)} records")
                print(f"  Columns: {list(fused_df.columns)}")
            else:
                print(f"  ‚ö†Ô∏è  {city}: No data fused")
        except Exception as e:
            print(f"  ‚ùå Error fusing {city}: {e}")
    
    print("\n" + "="*50)
    print("üéâ PIPELINE COMPLETED SUCCESSFULLY!")
    print("="*50)
    print("\nGenerated files:")
    
    # List generated files
    processed_dir = Path('data/processed')
    if processed_dir.exists():
        print("\nüìÅ Processed files:")
        for file in processed_dir.glob('*.parquet'):
            print(f"  - {file.name}")
    
    fused_dir = Path('data/fused')
    if fused_dir.exists():
        print("\nüìÅ Fused files:")
        for file in fused_dir.glob('*.parquet'):
            print(f"  - {file.name}")

if __name__ == "__main__":
    main()